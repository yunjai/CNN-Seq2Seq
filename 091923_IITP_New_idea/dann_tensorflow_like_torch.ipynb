{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nelec = pd.read_csv('nelec_082323.csv')\n",
    "dhw = pd.read_csv('dhw_merge.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nelec = nelec.drop(['YEAR'], axis=1)\n",
    "dhw = dhw.drop(['YEAR'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nelec = nelec.iloc[:-1,].reset_index(drop=True)\n",
    "output_nelec = nelec[['n_elec']].iloc[1:].reset_index(drop=True)\n",
    "output_nelec.columns = ['nelec']\n",
    "\n",
    "input_dhw = dhw.iloc[:-1,].reset_index(drop=True)\n",
    "output_dhw = dhw[['DHW']].iloc[1:].reset_index(drop=True)\n",
    "output_dhw.columns = ['dhw']\n",
    "\n",
    "nelec2 = pd.concat([input_nelec, output_nelec], axis=1)\n",
    "dhw2 = pd.concat([input_dhw, output_dhw], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_nelec = MinMaxScaler()\n",
    "scaler_dhw = MinMaxScaler()\n",
    "\n",
    "scaler_nelec.fit(nelec)\n",
    "scaler_dhw.fit(dhw)\n",
    "\n",
    "scaled_nelec = scaler_nelec.transform(nelec)\n",
    "scaled_dhw = scaler_dhw.transform(dhw)\n",
    "\n",
    "nelec = pd.DataFrame(scaled_nelec, index=nelec.index, columns=nelec.columns)\n",
    "dhw = pd.DataFrame(scaled_dhw, index=dhw.index, columns=dhw.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_nelec = int(len(nelec) * 0.7)\n",
    "standard_dhw = int(len(dhw) * 0.7)\n",
    "\n",
    "nelec_train = nelec.iloc[:standard_nelec]\n",
    "nelec_test = nelec.iloc[standard_nelec:].reset_index(drop=True)\n",
    "\n",
    "dhw_train = dhw.iloc[:standard_dhw]\n",
    "dhw_test = dhw.iloc[standard_dhw:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20417, 12)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nelec_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8751, 12)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nelec_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nelec_trainx = nelec_train.drop(['n_elec'], axis=1)\n",
    "nelec_trainy = nelec_train[['n_elec']]\n",
    "\n",
    "nelec_testx = nelec_test.drop(['n_elec'], axis=1)\n",
    "nelec_testy = nelec_test[['n_elec']]\n",
    "\n",
    "dhw_trainx = dhw_train.drop(['DHW'], axis=1)\n",
    "dhw_trainy = dhw_train[['DHW']]\n",
    "\n",
    "dhw_testx = dhw_test.drop(['DHW'], axis=1)\n",
    "dhw_testy = dhw_test[['DHW']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqLength = 24\n",
    "def buildDataSet(input, target, seqLength):\n",
    "    xdata = []\n",
    "    ydata = []\n",
    "    for i in range(len(input) - seqLength):\n",
    "        tx = input.iloc[i:i+seqLength]\n",
    "        ty = target.iloc[i+seqLength-1]\n",
    "        xdata.append(tx)\n",
    "        ydata.append(ty)\n",
    "    return np.array(xdata), np.array(ydata)\n",
    "\n",
    "nelec_trainx, nelec_trainy = buildDataSet(nelec_trainx, nelec_trainy, seqLength)\n",
    "nelec_testx, nelec_testy = buildDataSet(nelec_testx, nelec_testy, seqLength)\n",
    "\n",
    "dhw_trainx, dhw_trainy = buildDataSet(dhw_trainx, dhw_trainy, seqLength)\n",
    "dhw_testx, dhw_testy = buildDataSet(dhw_testx, dhw_testy, seqLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20393, 24, 11)\n",
      "(20393, 1)\n",
      "(8727, 24, 11)\n",
      "(8727, 1)\n",
      "(20393, 24, 11)\n",
      "(20393, 1)\n",
      "(8727, 24, 11)\n",
      "(8727, 1)\n"
     ]
    }
   ],
   "source": [
    "print(nelec_trainx.shape)\n",
    "print(nelec_trainy.shape)\n",
    "print(nelec_testx.shape)\n",
    "print(nelec_testy.shape)\n",
    "print(dhw_trainx.shape)\n",
    "print(dhw_trainy.shape)\n",
    "print(dhw_testx.shape)\n",
    "print(dhw_testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NumPy arrays to TensorFlow tensors\n",
    "nelec_trainx_tensor = tf.constant(nelec_trainx, dtype=tf.float32)\n",
    "nelec_trainy_tensor = tf.constant(nelec_trainy, dtype=tf.float32)\n",
    "\n",
    "nelec_testx_tensor = tf.constant(nelec_testx, dtype=tf.float32)\n",
    "nelec_testy_tensor = tf.constant(nelec_testy, dtype=tf.float32)\n",
    "\n",
    "dhw_trainx_tensor = tf.constant(dhw_trainx, dtype=tf.float32)\n",
    "dhw_trainy_tensor = tf.constant(dhw_trainy, dtype=tf.float32)\n",
    "\n",
    "dhw_testx_tensor = tf.constant(dhw_testx, dtype=tf.float32)\n",
    "dhw_testy_tensor = tf.constant(dhw_testy, dtype=tf.float32)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "nelec_train = tf.data.Dataset.from_tensor_slices((nelec_trainx_tensor, nelec_trainy_tensor))\n",
    "nelec_test = tf.data.Dataset.from_tensor_slices((nelec_testx_tensor, nelec_testy_tensor))\n",
    "\n",
    "dhw_train = tf.data.Dataset.from_tensor_slices((dhw_trainx_tensor, dhw_trainy_tensor))\n",
    "dhw_test = tf.data.Dataset.from_tensor_slices((dhw_testx_tensor, dhw_testy_tensor))\n",
    "\n",
    "# Define batch sizes\n",
    "batch_size_nelec_train = 512\n",
    "batch_size_nelec_test = 512\n",
    "batch_size_dhw_train = 512\n",
    "batch_size_dhw_test = 512\n",
    "\n",
    "# Create data loaders\n",
    "NELEC_train = nelec_train.batch(batch_size_nelec_train).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "NELEC_test = nelec_test.batch(batch_size_nelec_test).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "DHW_train = dhw_train.batch(batch_size_dhw_train).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "DHW_test = dhw_test.batch(batch_size_dhw_test).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 구조 선언"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientReversalLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversalLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(GradientReversalLayer, self).__init__()\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.negative(x), x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM for Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm(tf.keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(lstm, self).__init__()\n",
    "        self.lstm1 = tf.keras.layers.LSTM(64, return_sequences=True, return_state=True)\n",
    "        self.bn = tf.keras.layers.BatchNormalization()\n",
    "        self.lstm2 = tf.keras.layers.LSTM(64, return_sequences=True)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(0.6)\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "\n",
    "    def call(self, x):\n",
    "        x, h0, c0 = self.lstm1(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.lstm2(x, initial_state=[h0, c0])\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x[:,-1,:]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class domain_regression(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(domain_regression, self).__init__()\n",
    "        self.regression_layer1 = tf.keras.layers.Dense(100)\n",
    "        self.regression_layer4 = tf.keras.layers.Dense(1)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.dropout = tf.keras.layers.Dropout(0.6)\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.bn1(self.regression_layer1(x))\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.regression_layer4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class domain_classfication(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(domain_classfication, self).__init__()\n",
    "        self.classification_layer0 = GradientReversalLayer()\n",
    "        self.classification_layer1 = tf.keras.layers.Dense(100)\n",
    "        self.classification_layer4 = tf.keras.layers.Dense(1)\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.dropout = tf.keras.layers.Dropout(0.6)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.classification_layer0(x)\n",
    "        x = self.classification_layer1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classification_layer4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the DANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DANN(tf.keras.Model):\n",
    "    def __init__(self, lstm):\n",
    "        super(DANN, self).__init__()\n",
    "        self.lstm = lstm\n",
    "        self.regression = domain_regression()\n",
    "        self.classification = domain_classfication()\n",
    "\n",
    "    def call(self, x):\n",
    "        feature = self.lstm(x)\n",
    "        reg_output = self.regression(feature)\n",
    "        cla_output = self.classification(feature)\n",
    "\n",
    "        return reg_output, cla_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DANNLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        super(DANNLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.reg_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "        self.cla_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "    def call(self, y_true, y_pred, domain_num):\n",
    "        reg_output, cla_output = y_pred\n",
    "        batch_size = tf.shape(reg_output)[0]\n",
    "        cla_target = tf.fill((batch_size, 1), tf.cast(domain_num, dtype=tf.float32))\n",
    "\n",
    "        reg_loss = self.reg_loss(y_true[1], reg_output)\n",
    "        cla_loss = self.cla_loss(cla_target, cla_output)\n",
    "        cla_loss2 = cla_loss * self.alpha\n",
    "\n",
    "        loss = reg_loss + cla_loss2\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "my_lstm = lstm()\n",
    "model = DANN(my_lstm)\n",
    "# 손실 함수 생성\n",
    "alpha_value = 1.0  # 원하는 alpha 값으로 설정\n",
    "loss_fn = DANNLoss(alpha=alpha_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"domain_classfication_3\" (type domain_classfication).\n\nLayer \"dense_14\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor: shape=(512, 64), dtype=float32, numpy=\narray([[-0.08159236, -0.        , -0.        , ..., -0.        ,\n        -0.        , -0.        ],\n       [-0.07610296, -0.        , -0.        , ..., -0.        ,\n        -0.        , -0.        ],\n       [-0.07097369, -0.        , -0.        , ..., -0.        ,\n        -0.        , -0.        ],\n       ...,\n       [-0.03693607, -0.        , -0.00799986, ..., -0.00270401,\n        -0.        , -0.        ],\n       [-0.03417342, -0.        , -0.00669707, ..., -0.00293911,\n        -0.        , -0.        ],\n       [-0.03219561, -0.        , -0.00472287, ..., -0.00241153,\n        -0.        , -0.        ]], dtype=float32)>, <tf.Tensor: shape=(512, 64), dtype=float32, numpy=\narray([[0.08159236, 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.07610296, 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.07097369, 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       ...,\n       [0.03693607, 0.        , 0.00799986, ..., 0.00270401, 0.        ,\n        0.        ],\n       [0.03417342, 0.        , 0.00669707, ..., 0.00293911, 0.        ,\n        0.        ],\n       [0.03219561, 0.        , 0.00472287, ..., 0.00241153, 0.        ,\n        0.        ]], dtype=float32)>]\n\nCall arguments received by layer \"domain_classfication_3\" (type domain_classfication):\n  • x=tf.Tensor(shape=(512, 64), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yunjae\\Yunjae\\091923_IITP_New_idea\\dann_tensorflow_like_torch.ipynb 셀 30\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yunjae/Yunjae/091923_IITP_New_idea/dann_tensorflow_like_torch.ipynb#X50sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m target_y \u001b[39m=\u001b[39m target_data[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yunjae/Yunjae/091923_IITP_New_idea/dann_tensorflow_like_torch.ipynb#X50sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/yunjae/Yunjae/091923_IITP_New_idea/dann_tensorflow_like_torch.ipynb#X50sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m source_result \u001b[39m=\u001b[39m model(source_x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yunjae/Yunjae/091923_IITP_New_idea/dann_tensorflow_like_torch.ipynb#X50sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m target_result \u001b[39m=\u001b[39m model(target_x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yunjae/Yunjae/091923_IITP_New_idea/dann_tensorflow_like_torch.ipynb#X50sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# Calculate losses\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yunjae\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;32mc:\\Users\\yunjae\\Yunjae\\091923_IITP_New_idea\\dann_tensorflow_like_torch.ipynb 셀 30\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yunjae/Yunjae/091923_IITP_New_idea/dann_tensorflow_like_torch.ipynb#X50sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m feature \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yunjae/Yunjae/091923_IITP_New_idea/dann_tensorflow_like_torch.ipynb#X50sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m reg_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregression(feature)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/yunjae/Yunjae/091923_IITP_New_idea/dann_tensorflow_like_torch.ipynb#X50sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m cla_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclassification(feature)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yunjae/Yunjae/091923_IITP_New_idea/dann_tensorflow_like_torch.ipynb#X50sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mreturn\u001b[39;00m reg_output, cla_output\n",
      "\u001b[1;32mc:\\Users\\yunjae\\Yunjae\\091923_IITP_New_idea\\dann_tensorflow_like_torch.ipynb 셀 30\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yunjae/Yunjae/091923_IITP_New_idea/dann_tensorflow_like_torch.ipynb#X50sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yunjae/Yunjae/091923_IITP_New_idea/dann_tensorflow_like_torch.ipynb#X50sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassification_layer0(x)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/yunjae/Yunjae/091923_IITP_New_idea/dann_tensorflow_like_torch.ipynb#X50sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclassification_layer1(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yunjae/Yunjae/091923_IITP_New_idea/dann_tensorflow_like_torch.ipynb#X50sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yunjae/Yunjae/091923_IITP_New_idea/dann_tensorflow_like_torch.ipynb#X50sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"domain_classfication_3\" (type domain_classfication).\n\nLayer \"dense_14\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor: shape=(512, 64), dtype=float32, numpy=\narray([[-0.08159236, -0.        , -0.        , ..., -0.        ,\n        -0.        , -0.        ],\n       [-0.07610296, -0.        , -0.        , ..., -0.        ,\n        -0.        , -0.        ],\n       [-0.07097369, -0.        , -0.        , ..., -0.        ,\n        -0.        , -0.        ],\n       ...,\n       [-0.03693607, -0.        , -0.00799986, ..., -0.00270401,\n        -0.        , -0.        ],\n       [-0.03417342, -0.        , -0.00669707, ..., -0.00293911,\n        -0.        , -0.        ],\n       [-0.03219561, -0.        , -0.00472287, ..., -0.00241153,\n        -0.        , -0.        ]], dtype=float32)>, <tf.Tensor: shape=(512, 64), dtype=float32, numpy=\narray([[0.08159236, 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.07610296, 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.07097369, 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       ...,\n       [0.03693607, 0.        , 0.00799986, ..., 0.00270401, 0.        ,\n        0.        ],\n       [0.03417342, 0.        , 0.00669707, ..., 0.00293911, 0.        ,\n        0.        ],\n       [0.03219561, 0.        , 0.00472287, ..., 0.00241153, 0.        ,\n        0.        ]], dtype=float32)>]\n\nCall arguments received by layer \"domain_classfication_3\" (type domain_classfication):\n  • x=tf.Tensor(shape=(512, 64), dtype=float32)"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "alpha = 1.0\n",
    "epochs = 200\n",
    "patience = 100\n",
    "counter = 10\n",
    "\n",
    "# Define the learning rate schedule\n",
    "initial_learning_rate = 0.001\n",
    "end_learning_rate = 0.0\n",
    "total_steps = epochs * len(NELEC_train)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.keras.optimizers.Adamax(learning_rate=0.001, clipnorm=0.01)\n",
    "\n",
    "# Initialize variables for early stopping\n",
    "best_loss = float('inf')\n",
    "best_model_weights = None\n",
    "\n",
    "# Training loop\n",
    "for i in range(1, epochs + 1):\n",
    "    total_loss = 0\n",
    "    reg_loss_total = 0\n",
    "    cla_loss_total = 0\n",
    "\n",
    "    for step, (source_data, target_data) in enumerate(zip(NELEC_train, DHW_train)):\n",
    "        source_x = source_data[0].numpy()  # Convert to NumPy array\n",
    "        source_y = source_data[1].numpy()\n",
    "        target_x = target_data[0].numpy()\n",
    "        target_y = target_data[1].numpy()\n",
    "\n",
    "        # Forward pass\n",
    "        source_result = model(source_x)\n",
    "        target_result = model(target_x)\n",
    "\n",
    "        # Calculate losses\n",
    "        source_loss, source_reg_loss, source_cla_loss = loss_fn(source_result, source_y, 0, alpha=alpha)  # Source domain label 0\n",
    "        target_loss, target_reg_loss, target_cla_loss = loss_fn(target_result, target_y, 1, alpha=alpha)  # Target domain label 1\n",
    "\n",
    "        loss = source_loss + target_loss\n",
    "        reg_loss = source_reg_loss + target_reg_loss\n",
    "        cla_loss = source_cla_loss + target_cla_loss\n",
    "\n",
    "        reg_loss_total += reg_loss\n",
    "        cla_loss_total += cla_loss\n",
    "        total_loss += loss\n",
    "\n",
    "        # Compute gradients and update weights\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # Early stopping check\n",
    "    if reg_loss_total / len(NELEC_train) < best_loss:\n",
    "        best_loss = reg_loss_total / len(NELEC_train)\n",
    "        best_model_weights = model.get_weights()\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f'Early stopping at epoch {i}')\n",
    "            break\n",
    "\n",
    "    print(f'Epoch : {i}, Total Avg Loss : {total_loss / len(NELEC_train):.4f}')\n",
    "    print(f'Avg Regression Loss : {reg_loss_total / len(NELEC_train):.4f}')\n",
    "    print(f'Avg Classification Loss : {cla_loss_total / len(NELEC_train):.4f}\\n')\n",
    "\n",
    "# Save the best model\n",
    "if best_model_weights is not None:\n",
    "    model.set_weights(best_model_weights)\n",
    "    model.save('091823_in_the_lab.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train 데이터로 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 최상의 소스 모델 불러오기\n",
    "best_source_model2 = dann(my_lstm).to(device)\n",
    "best_source_model2.load_state_dict(torch.load('091823_in_the_lab.pth'))\n",
    "best_source_model2.eval()  # 평가 모드로 설정\n",
    "\n",
    "s_pred = []  # MNIST 데이터셋의 예측값을 저장할 리스트\n",
    "t_pred = []  # SVHN 데이터셋의 예측값을 저장할 리스트\n",
    "s_real = []\n",
    "t_real = []\n",
    "s_d_pred = []\n",
    "t_d_pred = []\n",
    "\n",
    "for step, (source_data, target_data) in enumerate(zip(NELEC_train, DHW_train)):\n",
    "    sourcex = source_data[0].to(device)\n",
    "    sourcey = source_data[1].to(device)\n",
    "    targetx = target_data[0].to(device)\n",
    "    targety = target_data[1].to(device)\n",
    "\n",
    "    # 저장된 최상의 모델로 예측\n",
    "    source_domain_label_pred, source_pred = best_source_model2(sourcex)\n",
    "    target_domain_label_pred, target_pred = best_source_model2(targetx)\n",
    "\n",
    "    ## 예측값을 리스트에 저장\n",
    "    s_pred.extend(source_pred.detach().cpu().numpy())\n",
    "    t_pred.extend(target_pred.detach().cpu().numpy())\n",
    "    s_real.extend(sourcey.detach().cpu().numpy())\n",
    "    t_real.extend(targety.detach().cpu().numpy())\n",
    "    s_d_pred.extend(source_domain_label_pred.detach().cpu().numpy())\n",
    "    t_d_pred.extend(target_domain_label_pred.detach().cpu().numpy())\n",
    "\n",
    "# R2 스코어 계산\n",
    "s_r2 = r2_score(s_real, s_pred)\n",
    "t_r2 = r2_score(t_real, t_pred)\n",
    "\n",
    "print('SOURCE R2 Score:', s_r2)\n",
    "print('TARGET R2 Score:', t_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,10))\n",
    "plt.plot(s_real, color='red', label='source real')\n",
    "plt.plot(s_pred, color='blue', label='source pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,10))\n",
    "plt.plot(t_real, color='red', label='target real')\n",
    "plt.plot(t_pred, color='blue', label='target pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test 데이터로 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 최상의 소스 모델 불러오기\n",
    "best_source_model2 = dann(my_lstm).to(device)\n",
    "best_source_model2.load_state_dict(torch.load('091823_in_the_lab.pth'))\n",
    "best_source_model2.eval()  # 평가 모드로 설정\n",
    "\n",
    "s_pred = []  # MNIST 데이터셋의 예측값을 저장할 리스트\n",
    "t_pred = []  # SVHN 데이터셋의 예측값을 저장할 리스트\n",
    "s_real = []\n",
    "t_real = []\n",
    "\n",
    "\n",
    "for step, (source_data, target_data) in enumerate(zip(NELEC_test, DHW_test)):\n",
    "    sourcex = source_data[0].to(device)\n",
    "    sourcey = source_data[1].to(device)\n",
    "    targetx = target_data[0].to(device)\n",
    "    targety = target_data[1].to(device)\n",
    "\n",
    "    # 저장된 최상의 모델로 예측\n",
    "    _, source_pred = best_source_model2(sourcex)\n",
    "    _, target_pred = best_source_model2(targetx)\n",
    "\n",
    "    ## 예측값을 리스트에 저장\n",
    "    s_pred.extend(source_pred.detach().cpu().numpy())\n",
    "    t_pred.extend(target_pred.detach().cpu().numpy())\n",
    "    s_real.extend(sourcey.detach().cpu().numpy())\n",
    "    t_real.extend(targety.detach().cpu().numpy())\n",
    "\n",
    "# R2 스코어 계산\n",
    "s_r2 = r2_score(s_real, s_pred)\n",
    "t_r2 = r2_score(t_real, t_pred)\n",
    "\n",
    "print('SOURCE R2 Score:', s_r2)\n",
    "print('TARGET R2 Score:', t_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,10))\n",
    "plt.plot(s_real, color='red', label='source real')\n",
    "plt.plot(s_pred, color='blue', label='source pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,10))\n",
    "plt.plot(t_real, color='red', label='target real')\n",
    "plt.plot(t_pred, color='blue', label='target pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound as sd\n",
    "def beepsound():\n",
    "    fr = 2000    # range : 37 ~ 32767\n",
    "    du = 800     # 1000 ms ==1second\n",
    "    sd.Beep(fr, du) # winsound.Beep(frequency, duration)\n",
    "beepsound()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Embedding Space 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 앞 batch의 250개씩의 데이터만 샘플링\n",
    "source_tsne = DataLoader(nelec_train,\n",
    "                        batch_size=1000,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "target_tsne = DataLoader(dhw_train,\n",
    "                        batch_size=1000,\n",
    "                        shuffle=False,  \n",
    "                        drop_last=True)\n",
    "\n",
    "source_tsne2 = next(iter(source_tsne))\n",
    "target_tsne2 = next(iter(target_tsne))\n",
    "\n",
    "source_x_TNSE = source_tsne2[0].to(device)\n",
    "source_y_TNSE = source_tsne2[1].to(device)\n",
    "\n",
    "target_x_TNSE = target_tsne2[0].to(device)\n",
    "target_y_TNSE = target_tsne2[1].to(device)\n",
    "\n",
    "# 학습된 모델의 LSTM 부분만 활용 (100차원 임베딩 벡터를 받아오는 과정)\n",
    "source_vector = model.lstm(source_x_TNSE)\n",
    "target_vector = model.lstm(target_x_TNSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "df = pd.DataFrame(np.concatenate([source_vector.cpu().detach().numpy(), target_vector.cpu().detach().numpy()]))\n",
    "\n",
    "tsne_np = TSNE(n_components=2).fit_transform(df)\n",
    "tsne_df = pd.DataFrame(tsne_np, columns=['component 0', 'component 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df_0 = tsne_df.loc[:1000]\n",
    "tsne_df_1 = tsne_df.loc[1000:]\n",
    "\n",
    "plt.scatter(tsne_df_0['component 0'], tsne_df_0['component 1'], color='green', label='NELEC', alpha=0.5)\n",
    "plt.scatter(tsne_df_1['component 0'], tsne_df_1['component 1'], color='black', label='DHW', alpha=0.5)\n",
    "\n",
    "plt.title('alpha = '+ str(alpha))\n",
    "plt.xlabel('component 0')\n",
    "plt.ylabel('component 1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
